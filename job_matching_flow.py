import os
import json
import requests
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
from typing import List, Dict, Any
from prompts import MATCHING_PROMPT

# Load environment variables
load_dotenv()

# Constants
GAS_URL = "https://script.google.com/macros/s/AKfycbz2_SYNhkbrBjp1Zv7zB2tQKesGNNtjQGgFBLsl7DmLd3PohCFBG0ZT9ojNReBXa2Zv/exec"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX_HOST = os.getenv("PINECONE_INDEX_HOST")

# LLMè¨­å®š - é€Ÿåº¦æ”¹å–„ã®ãŸã‚ã®ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œ
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "openai")  # openai, ai_studio, bedrock
LLM_MODEL = os.getenv("LLM_MODEL")  # ãƒ¢ãƒ‡ãƒ«åï¼ˆçœç•¥æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")  # Google AI Studio Gemini API Key
AWS_REGION = os.getenv("AWS_REGION", "ap-northeast-1")

# èµ·å‹•æ™‚ã®ç’°å¢ƒå¤‰æ•°ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
print(f"ğŸ”§ [STARTUP] Environment Variables:")
print(f"   LLM_PROVIDER: {LLM_PROVIDER}")
print(f"   LLM_MODEL: {LLM_MODEL}")
print(f"   GEMINI_API_KEY: {'***' + GEMINI_API_KEY[-4:] if GEMINI_API_KEY else 'None'}")
print(f"   OPENAI_API_KEY: {'***' + OPENAI_API_KEY[-4:] if OPENAI_API_KEY else 'None'}")

# Initialize LangChain components (é…å»¶åˆæœŸåŒ–)
llm = None
embeddings = None
vectorstore = None

def get_llm(provider=None):
    """é«˜é€ŸåŒ–å¯¾å¿œã®ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼LLMå–å¾—é–¢æ•°"""
    global llm
    if llm is None:
        from llm_config import get_model_config
        
        current_provider = provider or LLM_PROVIDER
        model_config = get_model_config(current_provider, LLM_MODEL)
        
        print(f"ğŸš€ [LLM_INIT] Provider: {current_provider}, Model: {model_config['model']}")
        
        if current_provider == "ai_studio":
            try:
                from langchain_google_genai import ChatGoogleGenerativeAI
                llm = ChatGoogleGenerativeAI(
                    model=model_config["model"],
                    temperature=model_config["temperature"],
                    google_api_key=GEMINI_API_KEY
                )
                print(f"âœ… [SUCCESS] Using Google AI Studio {model_config['model']}: {model_config['description']}")
            except ImportError:
                print("âŒ [ERROR] langchain-google-genai not installed. Falling back to OpenAI.")
                current_provider = "openai"
                model_config = get_model_config("openai", "gpt4o_mini")
        
        elif current_provider == "bedrock":
            try:
                from langchain_aws import ChatBedrock
                llm = ChatBedrock(
                    model_id=model_config["model_id"],
                    region_name=model_config.get("region_name", AWS_REGION),
                    model_kwargs={"temperature": model_config["temperature"]}
                )
                print(f"âœ… [SUCCESS] Using AWS Bedrock {model_config['model_id']}: {model_config['description']}")
            except ImportError:
                print("âŒ [ERROR] langchain-aws not installed. Falling back to OpenAI.")
                current_provider = "openai"
                model_config = get_model_config("openai", "gpt4o_mini")
        
        if current_provider == "openai":
            llm = ChatOpenAI(
                model=model_config["model"], 
                temperature=model_config["temperature"], 
                api_key=OPENAI_API_KEY
            )
            print(f"âœ… [FALLBACK] Using OpenAI {model_config['model']}: {model_config['description']}")
    
    return llm

def get_embeddings():
    global embeddings
    if embeddings is None:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=OPENAI_API_KEY)
    return embeddings

def get_vectorstore():
    global vectorstore
    if vectorstore is None:
        vectorstore = PineconeVectorStore.from_existing_index(
            index_name="yoin-index",
            embedding=get_embeddings(),
            pinecone_api_key=PINECONE_API_KEY,
            host=PINECONE_INDEX_HOST
        )
    return vectorstore

# Pydantic models for structured output
class YoinStructured(BaseModel):
    id: str = Field(description="ID")
    date: str = Field(description="å—ä¿¡æ—¥æ™‚")
    name: str = Field(description="æ°å")
    age: str = Field(description="å¹´é½¢")
    skill: str = Field(description="ã‚¹ã‚­ãƒ«")
    station: str = Field(description="æœ€å¯„é§…")
    work_style: str = Field(description="å‹¤å‹™å½¢æ…‹")
    price: str = Field(description="å˜ä¾¡")
    etc: str = Field(description="å‚™è€ƒ")
    subject: str = Field(description="ä»¶å")

class AnkenStructured(BaseModel):
    id: str = Field(description="ID")
    date: str = Field(description="å—ä¿¡æ—¥æ™‚")
    name: str = Field(description="æ¡ˆä»¶å")
    skill: str = Field(description="å¿…é ˆã‚¹ã‚­ãƒ«")
    station: str = Field(description="ä½œæ¥­å ´æ‰€")
    work_style: str = Field(description="å‹¤å‹™å½¢æ…‹")
    schedule: str = Field(description="æœŸé–“")
    price: str = Field(description="å˜ä¾¡")
    etc: str = Field(description="å‚™è€ƒ")
    subject: str = Field(description="ä»¶å")

class MatchCandidate(BaseModel):
    yoin_id: str
    date: str
    yoin_info: Dict[str, Any]
    match_score: int
    comment: str

class MatchingResult(BaseModel):
    candidates: List[MatchCandidate]
    comparision: List[Dict[str, Any]]
    actions: List[str]

def get_data_from_gas(type_: str, params: Dict[str, Any] = None) -> Dict:
    """GASã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    url = f"{GAS_URL}?type={type_}"
    if params:
        param_str = "&".join([f"{k}={v}" for k, v in params.items() if v])
        url += f"&{param_str}"
    
    response = requests.get(url)
    response.raise_for_status()
    return response.json()

def post_data_to_gas(type_: str, data: Dict) -> Dict:
    """GASã«ãƒ‡ãƒ¼ã‚¿ã‚’POST"""
    print("data", data)
    url = f"{GAS_URL}?type={type_}"
    response = requests.post(url, json=data)
    response.raise_for_status()
    return response.json()

def format_yoin_item(item: Dict) -> str:
    """è¦å“¡ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ãƒ†ã‚­ã‚¹ãƒˆã«æ•´å½¢"""
    return f"""
ä»¥ä¸‹ã¯è¦å“¡æƒ…å ±ãƒ¡ãƒ¼ãƒ«ã§ã™ã€‚
ID: {item.get('ID', '')}
å—ä¿¡æ—¥æ™‚: {item.get('å—ä¿¡æ—¥æ™‚', '')}
ä»¶å: {item.get('ä»¶å', '')}
æœ¬æ–‡:
{item.get('æœ¬æ–‡', '')}
"""

def format_anken_item(item: Dict) -> str:
    """æ¡ˆä»¶ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ãƒ†ã‚­ã‚¹ãƒˆã«æ•´å½¢"""
    return f"""
ä»¥ä¸‹ã¯æ¡ˆä»¶æƒ…å ±ãƒ¡ãƒ¼ãƒ«ã§ã™ã€‚
ID: {item.get('ID', '')}
å—ä¿¡æ—¥æ™‚: {item.get('å—ä¿¡æ—¥æ™‚', '')}
ä»¶å: {item.get('ä»¶å', '')}
æœ¬æ–‡:
{item.get('æœ¬æ–‡', '')}
"""

def structure_yoin_data(raw_text: str, item: Dict) -> Dict:
    """è¦å“¡ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–"""
    prompt_text = f"""
ã‚ãªãŸã¯ITã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢æ´¾é£ã®ã€Œè¦å“¡æƒ…å ±ã€ã‚’æ•´ç†ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã«å…¥åŠ›ã¨ã—ã¦ã€Œãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã€ã€Œãƒ¡ãƒ¼ãƒ«IDã€ã€Œå—ä¿¡æ—¥æ™‚ã€ã€Œä»¶åã€ã€Œæœ¬æ–‡ã€ãŒä¸ãˆã‚‰ã‚Œã¾ã™ã€‚
ã“ã‚Œã‚‰ã®æƒ…å ±ã‚’ã‚‚ã¨ã«ã€è¦å“¡æƒ…å ±ã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

å…¥åŠ›æƒ…å ±:
{raw_text}

å¿…ãšæ¬¡ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

{{
  "id": "ï¼ˆã“ã“ã«ä¸Šè¨˜ã®idã‚’ãã®ã¾ã¾è¨˜å…¥ï¼‰",
  "date": "ï¼ˆã“ã“ã«ä¸Šè¨˜ã®å—ä¿¡æ—¥æ™‚ã‚’ãã®ã¾ã¾è¨˜å…¥ï¼‰",
  "name": "ï¼ˆã“ã“ã«è¦å“¡ã®ã‚¤ãƒ‹ã‚·ãƒ£ãƒ«ã‚’è¨˜å…¥ï¼‰",
  "age":"ï¼ˆã“ã“ã«è¦å“¡ã®å¹´é½¢ã‚’è¨˜å…¥ï¼‰",
  "skill": "ï¼ˆã“ã“ã«è¦å“¡ã®ã‚¹ã‚­ãƒ«ã‚’è¨˜å…¥ï¼‰",
  "station": "ï¼ˆã“ã“ã«è¦å“¡ã®æœ€å¯„é§…ï¼‰",
  "work_style": "ï¼ˆã“ã“ã«è¦å“¡ã®å¸Œæœ›å‹¤å‹™å½¢æ…‹ã‚’è¨˜å…¥ï¼‰",
  "price": "ï¼ˆã“ã“ã«è¦å“¡ã®å¸Œæœ›å˜ä¾¡ã‚’è¨˜å…¥ï¼‰",
  "etc": "ï¼ˆã“ã“ã«ãã®ä»–å‚™è€ƒã‚’è¨˜å…¥ï¼‰",
  "subject": ""
}}

- å‡ºåŠ›ã¯å¿…ãšä¸Šè¨˜ã®JSONã®ã¿ã§è¿”ã—ã¦ãã ã•ã„ã€‚
- "skill"ã¯ç®‡æ¡æ›¸ãã«å¤‰æ›ã—ã€å„è¡Œã®é ­ã«"- "ã‚’ä»˜åŠ ã—ã¦ãã ã•ã„
- "name"ã¯ã€ŒA.Bã€ã®å½¢å¼ã§ã€ã‚¤ãƒ‹ã‚·ãƒ£ãƒ«ä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
- ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚„èª¬æ˜æ–‡ã¯ä¸è¦ã§ã™ã€‚
- "age"ã¯ã€Œæ­³ã€ã€Œæ‰ã€ãªã©ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
- idã¨dateã¯çµ¶å¯¾ã«å…¥åŠ›å€¤ã¨ä¸€è‡´ã•ã›ã¦ãã ã•ã„ã€‚
"""
    
    # LLMã‚’ç›´æ¥å‘¼ã³å‡ºã—
    response = get_llm().invoke(prompt_text)
    
    # JSONãƒ‘ãƒ¼ã‚¹ã¨Pydanticãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    try:
        # JSONæ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹
        json_data = json.loads(response.content)
        # Pydanticãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        result = YoinStructured(**json_data)
        return result.model_dump()
    except Exception as e:
        return {"error": f"Parse failed: {e}", "raw_response": response.content}

def structure_anken_data(raw_text: str, item: Dict) -> Dict:
    """æ¡ˆä»¶ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–"""
    prompt_text = f"""
ã‚ãªãŸã¯ITã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢æ´¾é£ã®ã€Œæ¡ˆä»¶æƒ…å ±ã€ã‚’æ•´ç†ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã«å…¥åŠ›ã¨ã—ã¦ã€Œãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã€ã€Œãƒ¡ãƒ¼ãƒ«IDã€ã€Œå—ä¿¡æ—¥æ™‚ã€ã€Œä»¶åã€ã€Œæœ¬æ–‡ã€ãŒä¸ãˆã‚‰ã‚Œã¾ã™ã€‚
ã“ã‚Œã‚‰ã®æƒ…å ±ã‚’ã‚‚ã¨ã«ã€æ¡ˆä»¶æƒ…å ±ã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

å…¥åŠ›æƒ…å ±:
{raw_text}

å¿…ãšæ¬¡ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

{{
  "id": "ï¼ˆã“ã“ã«ä¸Šè¨˜ã®idã‚’ãã®ã¾ã¾è¨˜å…¥ï¼‰",
  "date": "ï¼ˆã“ã“ã«ä¸Šè¨˜ã®å—ä¿¡æ—¥æ™‚ã‚’ãã®ã¾ã¾è¨˜å…¥ï¼‰",
  "name": "ï¼ˆã“ã“ã«æ¡ˆä»¶åã‚’è¨˜å…¥ï¼‰",
  "skill": "",
  "station": "ï¼ˆã“ã“ã«å‹¤å‹™åœ°ã®æœ€å¯„é§…ã‚’è¨˜å…¥ï¼‰",
  "work_style": "ï¼ˆã“ã“ã«å‹¤å‹™å½¢æ…‹ã‚’è¨˜å…¥ï¼‰",
  "schedule": "",
  "price": "ï¼ˆã“ã“ã«å˜ä¾¡ã€å˜é‡‘ã‚’è¨˜å…¥ï¼‰",
  "etc": "ï¼ˆã“ã“ã«ãã®ä»–å‚™è€ƒã‚’è¨˜å…¥ï¼‰",
  "subject": "ï¼ˆã“ã“ã«ä»¶åã‚’è¨˜å…¥ï¼‰"
}}

æ³¨æ„:
- "skill"ã¯è¦å“¡ã®ã‚¹ã‚­ãƒ«ã‚’ç®‡æ¡æ›¸ãã«å¤‰æ›ã—ã€å„è¡Œã®é ­ã«"- "ã‚’ä»˜åŠ ã—ã¦ãã ã•ã„
- "schedule"ã¯ã€ŒæœŸé–“ã€ã€Œæ™‚æœŸã€ã€Œé–‹å§‹ã€ã€Œä½œæ¥­é–‹å§‹æ™‚æœŸã€ã€Œä½œæ¥­æœŸé–“ã€ã€Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ãªã©ã§ç¤ºã•ã‚Œã¦ã„ã‚‹
- å‡ºåŠ›ã¯å¿…ãšä¸Šè¨˜ã®JSONã®ã¿ã§è¿”ã—ã¦ãã ã•ã„ã€‚
- ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚„èª¬æ˜æ–‡ã¯ä¸è¦ã§ã™ã€‚
- idã¨dateã¯çµ¶å¯¾ã«å…¥åŠ›å€¤ã¨ä¸€è‡´ã•ã›ã¦ãã ã•ã„ã€‚
"""
    
    # LLMã‚’ç›´æ¥å‘¼ã³å‡ºã—
    response = get_llm().invoke(prompt_text)
    
    # JSONãƒ‘ãƒ¼ã‚¹ã¨Pydanticãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    try:
        # JSONæ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹
        json_data = json.loads(response.content)
        # Pydanticãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        result = AnkenStructured(**json_data)
        return result.model_dump()
    except Exception as e:
        return {"error": f"Parse failed: {e}", "raw_response": response.content}

# æ§‹é€ åŒ–ãƒ•ãƒ­ãƒ¼ï¼ˆè¦å“¡ï¼‰
def format_yoin_flow(params: Dict[str, Any]):
    """æ§‹é€ åŒ–ãƒ•ãƒ­ãƒ¼ï¼ˆè¦å“¡ï¼‰"""
    print("Starting format_yoin flow...")
    
    # with_indexãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯
    with_index = params.get("with_index", False)
    if isinstance(with_index, str):
        with_index = with_index.lower() == "true"
    
    # Get data from GAS
    data = get_data_from_gas("yoin", params)
    records = data.get("records", [])
    
    for record in records:
        # Format item
        formatted_text = format_yoin_item(record)
        
        # Structure with LLM
        structured = structure_yoin_data(formatted_text, record)
        
        # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if "error" in structured:
            print(f"Error processing record {record.get('ID', 'unknown')}: {structured['error']}")
            print(f"Raw response: {structured.get('raw_response', '')}")
            continue
        
        # Post back to GAS
        # raw_inputãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ ã—ã¦ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã‚’Kåˆ—ã«ä¿å­˜
        structured["raw_input"] = record.get('æœ¬æ–‡', '')
        post_data = {
            "type": "yoin",
            "record": structured
        }
        post_data_to_gas("yoin", post_data)
        print(f"Processed yoin ID: {structured.get('id', 'unknown')}")
    
    print("format_yoin flow completed.")
    
    # with_index=Trueã®å ´åˆã€ç¶šã‘ã¦RAGç™»éŒ²ã‚’å®Ÿè¡Œ
    if with_index:
        print("\n--- Starting automatic index_yoin flow ---")
        # paramsã‹ã‚‰with_indexã‚’é™¤å¤–ã—ã¦index_yoin_flowã«æ¸¡ã™
        index_params = {k: v for k, v in params.items() if k != "with_index"}
        index_yoin_flow(index_params)
        
# æ§‹é€ åŒ–ãƒ•ãƒ­ãƒ¼ï¼ˆæ¡ˆä»¶ï¼‰
def format_anken_flow(params: Dict[str, Any]):
    """æ§‹é€ åŒ–ãƒ•ãƒ­ãƒ¼ï¼ˆæ¡ˆä»¶ï¼‰"""
    print("Starting format_anken flow...")
    
    # Get data from GAS
    data = get_data_from_gas("anken", params)

    records = data.get("records", [])
    
    for record in records:
        raw_text = record["æœ¬æ–‡"]
        # Format item
        formatted_text = format_anken_item(record)

        # print(formatted_text)
        
        # Structure with LLM
        structured = structure_anken_data(formatted_text, record)
        
        # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if "error" in structured:
            print(f"Error processing record {record.get('ID', 'unknown')}: {structured['error']}")
            print(f"Raw response: {structured.get('raw_response', '')}")
            continue
        
        # raw_inputãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ ã—ã¦ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã‚’Kåˆ—ã«ä¿å­˜
        structured["raw_input"] = record.get('æœ¬æ–‡', '')

        print(structured)
        
        # Post back to GAS
        post_data = {
            "type": "anken",
            "record": structured
        }
        post_data_to_gas("anken", post_data)
        print(f"Processed anken ID: {structured.get('id', 'unknown')}")
    
    print("format_anken flow completed.")

# RAGç™»éŒ²ãƒ•ãƒ­ãƒ¼ï¼ˆè¦å“¡ï¼‰
def index_yoin_flow(params: Dict[str, Any]):
    """RAGç™»éŒ²ãƒ•ãƒ­ãƒ¼ï¼ˆè¦å“¡ï¼‰"""
    print("Starting index_yoin flow...")
    
    # Get formatted data from GAS
    data = get_data_from_gas("yoin_format", params)
    records = data.get("records", [])
    
    # Initialize Pinecone vector store
    vectorstore = PineconeVectorStore(
        index_name="yoin2",  # Assuming index name from host
        embedding=get_embeddings(),
        pinecone_api_key=PINECONE_API_KEY
    )
    
    for record in records:
        # Create RAG text
        rag_text = f"""
        ã€è¦å“¡IDã€‘ {record.get('ID', '')}
        ã€å—ä¿¡æ—¥æ™‚ã€‘ {record.get('å—ä¿¡æ—¥æ™‚', '')}
        ã€æ°åã€‘ {record.get('æ°å', '')}
        ã€å¹´é½¢ã€‘ {record.get('å¹´é½¢', '')}
        ã€ã‚¹ã‚­ãƒ«ã€‘ {record.get('ã‚¹ã‚­ãƒ«', '')}
        ã€æœ€å¯„é§…ã€‘ {record.get('æœ€å¯„é§…', '')}
        ã€å‹¤å‹™å½¢æ…‹ï¼ˆå¸Œæœ›ï¼‰ã€‘ {record.get('å‹¤å‹™å½¢æ…‹ï¼ˆå¸Œæœ›ï¼‰', '')}
        ã€å˜ä¾¡ï¼ˆå¸Œæœ›ï¼‰ã€‘ {record.get('å˜ä¾¡ï¼ˆå¸Œæœ›ï¼‰', '')}
        ã€å‚™è€ƒã€‘ {record.get('å‚™è€ƒ', '')}
        ã€ãƒ¡ãƒ¼ãƒ«ã‚¿ã‚¤ãƒˆãƒ«ã€‘ {record.get('ãƒ¡ãƒ¼ãƒ«ã‚¿ã‚¤ãƒˆãƒ«', '')}
        """
        
        # Add to vector store
        vectorstore.add_texts(
            texts=[rag_text.strip()],
            ids=[record.get('ID', '')],
            metadatas=[{
                "recieved_at": int(record.get('å—ä¿¡æ—¥æ™‚', '').replace('-', '')[:8]),
                "text": rag_text.strip()
            }]
        )
        print(f"Indexed yoin ID: {record.get('ID', '')}")
    
    print("index_yoin flow completed.")

# è¦å“¡ãƒãƒƒãƒãƒ•ãƒ­ãƒ¼
def matching_yoin_flow(anken: str):
    """è¦å“¡ãƒãƒƒãƒãƒ•ãƒ­ãƒ¼"""
    print("Starting matching_yoin flow...")
    
    # Parse anken data
    anken_data = json.loads(anken)
    
    # Create search text with weighted keywords (improved approach)
    é‡ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ = anken_data.get('é‡ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', '')
    
    # ã‚ˆã‚Šè‡ªç„¶ãªå½¢ã§é‡è¦åº¦ã‚’é«˜ã‚ã‚‹
    search_text = f"""
ã€æœ€é‡è¦ã‚¹ã‚­ãƒ«ã€‘: {é‡ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰}
æ¡ˆä»¶å: {anken_data.get('æ¡ˆä»¶å', '')}
æ±‚ã‚ã‚‹ã‚¹ã‚­ãƒ«: {é‡ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰}
å¿…é ˆã‚¹ã‚­ãƒ«: {anken_data.get('å¿…é ˆã‚¹ã‚­ãƒ«', '')}
ä½œæ¥­å ´æ‰€: {anken_data.get('ä½œæ¥­å ´æ‰€', '')}
å˜ä¾¡: {anken_data.get('å˜ä¾¡', '')}
å‚™è€ƒ: {anken_data.get('å‚™è€ƒ', '')}
å„ªå…ˆæŠ€è¡“: {é‡ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰}
""".strip()
    
    # Initialize Pinecone vector store
    vectorstore = PineconeVectorStore(
        index_name="yoin2",
        embedding=get_embeddings(),
        pinecone_api_key=PINECONE_API_KEY
    )
    
    # Search similar vectors
    docs = vectorstore.similarity_search_with_score(search_text, k=20)
    
    # Format results for LLM
    matches_text = ""
    if docs:
        for doc, score in docs:
            print("debug")
            print(doc.page_content)
            matches_text += f"""
â–  è¦å“¡ID: {doc.id}
ã‚¹ã‚³ã‚¢: {score}
{doc.page_content}
-------------------------
""".strip() + "\n"
    else:
        matches_text = "æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    # LLM matching - ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œ
    prompt = PromptTemplate.from_template(MATCHING_PROMPT)

    print(f"debug - Using LLM Provider: {LLM_PROVIDER}, Model: {LLM_MODEL or 'default'}")
    print("debug" + MATCHING_PROMPT)
    
    chain = prompt | get_llm() | JsonOutputParser(pydantic_object=MatchingResult)
    result = chain.invoke({
        "anken_formatted": json.dumps(anken_data, ensure_ascii=False),
        "matches_text": matches_text
    })
    
    print("Matching result:")
    print(json.dumps(result, ensure_ascii=False, indent=2))
    print("matching_yoin flow completed.")
    return result

# è¦å“¡ãƒãƒƒãƒãƒ•ãƒ­ãƒ¼ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œãƒ»é«˜é€ŸåŒ–ç‰ˆï¼‰
async def matching_yoin_flow_stream(anken: str, mode: str = None):
    """è¦å“¡ãƒãƒƒãƒãƒ•ãƒ­ãƒ¼ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œãƒ»ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œï¼‰"""
    import asyncio
    
    yield {"type": "status", "message": f"æ¡ˆä»¶ãƒ‡ãƒ¼ã‚¿ã‚’è§£æä¸­... (Provider: {LLM_PROVIDER}, Model: {LLM_MODEL or 'default'})"}
    await asyncio.sleep(0.1)
    
    # Parse anken data
    anken_data = json.loads(anken)
    
    yield {"type": "status", "message": "æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ä½œæˆä¸­..."}
    await asyncio.sleep(0.1)
    
    # Create search text with weighted keywords
    é‡ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ = anken_data.get('é‡ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', '')
    
    search_text = f"""
ã€æœ€é‡è¦ã‚¹ã‚­ãƒ«ã€‘: {é‡ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰}
æ¡ˆä»¶å: {anken_data.get('æ¡ˆä»¶å', '')}
æ±‚ã‚ã‚‹ã‚¹ã‚­ãƒ«: {é‡ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰}
å¿…é ˆã‚¹ã‚­ãƒ«: {anken_data.get('å¿…é ˆã‚¹ã‚­ãƒ«', '')}
ä½œæ¥­å ´æ‰€: {anken_data.get('ä½œæ¥­å ´æ‰€', '')}
å˜ä¾¡: {anken_data.get('å˜ä¾¡', '')}
å‚™è€ƒ: {anken_data.get('å‚™è€ƒ', '')}
å„ªå…ˆæŠ€è¡“: {é‡ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰}
""".strip()
    
    yield {"type": "status", "message": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰è¦å“¡ã‚’æ¤œç´¢ä¸­..."}
    await asyncio.sleep(0.1)
    
    # Initialize Pinecone vector store
    vectorstore = PineconeVectorStore(
        index_name="yoin2",
        embedding=get_embeddings(),
        pinecone_api_key=PINECONE_API_KEY
    )
    
    # Search similar vectors
    docs = vectorstore.similarity_search_with_score(search_text, k=20)
    
    # è¦‹ã¤ã‹ã£ãŸè¦å“¡ã®IDãƒªã‚¹ãƒˆã‚’ä½œæˆ
    found_yoin_ids = [doc.id for doc, score in docs]
    
    yield {
        "type": "search_complete", 
        "message": f"{len(docs)}ä»¶ã®å€™è£œã‚’ç™ºè¦‹", 
        "count": len(docs),
        "yoin_ids": found_yoin_ids
    }
    await asyncio.sleep(0.1)
    
    # quickãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯æ¤œç´¢çµæœã®ã¿è¿”ã™
    if mode == "quick":
        formatted_results = []
        for i, (doc, score) in enumerate(docs):
            formatted_results.append({
                "id": doc.id,
                "content": doc.page_content,
                "score": float(score),
                "metadata": doc.metadata
            })
            yield {
                "type": "quick_result",
                "index": i + 1,
                "total": len(docs),
                "result": formatted_results[-1]
            }
            await asyncio.sleep(0.05)
        
        yield {
            "type": "final_result",
            "message": "æ¤œç´¢å®Œäº†",
            "results": {"quick_results": formatted_results}
        }
        return
    
    # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: LLMã§ã®ãƒãƒƒãƒãƒ³ã‚°åˆ†æï¼ˆé«˜é€ŸåŒ–å¯¾å¿œï¼‰
    yield {"type": "status", "message": f"AIåˆ†æã‚’é–‹å§‹ä¸­... (Using {LLM_PROVIDER}:{LLM_MODEL or 'default'})"}
    await asyncio.sleep(0.1)
    
    # Format results for LLM
    matches_text = ""
    if docs:
        for doc, score in docs:
            matches_text += f"""
â–  è¦å“¡ID: {doc.id}
ã‚¹ã‚³ã‚¢: {score}
{doc.page_content}
-------------------------
""".strip() + "\n"
    else:
        matches_text = "æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    yield {"type": "status", "message": "ãƒãƒƒãƒãƒ³ã‚°åˆ†æä¸­..."}
    await asyncio.sleep(0.1)
    
    # LLM matching with streaming - ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œ
    prompt = PromptTemplate.from_template(MATCHING_PROMPT)
    
    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®LLMä½œæˆï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
    from llm_config import get_model_config
    model_config = get_model_config(LLM_PROVIDER, LLM_MODEL)
    
    if LLM_PROVIDER == "ai_studio":
        try:
            from langchain_google_genai import ChatGoogleGenerativeAI
            llm_stream = ChatGoogleGenerativeAI(
                model=model_config["model"],
                temperature=model_config["temperature"],
                google_api_key=GEMINI_API_KEY,
                streaming=True
            )
        except ImportError:
            from langchain_openai import ChatOpenAI
            fallback_config = get_model_config("openai", "gpt4o_mini")
            llm_stream = ChatOpenAI(
                model=fallback_config["model"], 
                temperature=fallback_config["temperature"], 
                api_key=OPENAI_API_KEY,
                streaming=True
            )
    elif LLM_PROVIDER == "bedrock":
        try:
            from langchain_aws import ChatBedrock
            llm_stream = ChatBedrock(
                model_id=model_config["model_id"],
                region_name=model_config.get("region_name", AWS_REGION),
                model_kwargs={"temperature": model_config["temperature"]},
                streaming=True
            )
        except ImportError:
            from langchain_openai import ChatOpenAI
            fallback_config = get_model_config("openai", "gpt4o_mini")
            llm_stream = ChatOpenAI(
                model=fallback_config["model"], 
                temperature=fallback_config["temperature"], 
                api_key=OPENAI_API_KEY,
                streaming=True
            )
    else:  # OpenAI
        from langchain_openai import ChatOpenAI
        llm_stream = ChatOpenAI(
            model=model_config["model"], 
            temperature=model_config["temperature"], 
            api_key=OPENAI_API_KEY,
            streaming=True
        )
    
    chain = prompt | llm_stream
    
    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†
    accumulated_response = ""
    async for chunk in chain.astream({
        "anken_formatted": json.dumps(anken_data, ensure_ascii=False),
        "matches_text": matches_text
    }):
        if chunk.content:
            accumulated_response += chunk.content
            yield {
                "type": "llm_chunk",
                "content": chunk.content,
                "accumulated": accumulated_response
            }
    
    yield {"type": "status", "message": "åˆ†æçµæœã‚’æ•´ç†ä¸­..."}
    await asyncio.sleep(0.1)
    
    # Parse final JSON result
    try:
        # JSONãƒ‘ãƒ¼ã‚µãƒ¼ã§æœ€çµ‚çµæœã‚’è§£æ
        from langchain_core.output_parsers import JsonOutputParser
        parser = JsonOutputParser(pydantic_object=MatchingResult)
        result = parser.parse(accumulated_response)
        
        yield {
            "type": "final_result",
            "message": "ãƒãƒƒãƒãƒ³ã‚°åˆ†æå®Œäº†",
            "result": result
        }
    except Exception as e:
        yield {
            "type": "error",
            "message": f"çµæœã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
            "raw_response": accumulated_response
        }

def main(action: str, **kwargs):
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    params = {
        "start_date": kwargs.get("start_date"),
        "end_date": kwargs.get("end_date"),
        "limit": kwargs.get("limit"),
        "offset": kwargs.get("offset"),
        "with_index": kwargs.get("with_index")
    }
    
    if action == "format_yoin":
        format_yoin_flow(params)
    elif action == "format_anken":
        format_anken_flow(params)
    elif action == "index_yoin":
        index_yoin_flow(params)
    elif action == "matching_yoin":
        matching_yoin_flow(kwargs.get("anken", ""))
    elif action == "matching_yoin_stream":
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç‰ˆã®å®Ÿè¡Œï¼ˆasyncioå¯¾å¿œï¼‰
        import asyncio
        async def run_stream():
            async for chunk in matching_yoin_flow_stream(
                kwargs.get("anken", ""), 
                kwargs.get("mode", None)
            ):
                print(f"Stream chunk: {chunk}")
        
        asyncio.run(run_stream())
    else:
        print(f"Unknown action: {action}")

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: python job_matching_flow.py <action> [kwargs...]")
        print("Actions: format_yoin, format_anken, index_yoin, matching_yoin, matching_yoin_stream")
        sys.exit(1)
    
    action = sys.argv[1]
    kwargs = {}
    for arg in sys.argv[2:]:
        if "=" in arg:
            key, value = arg.split("=", 1)
            kwargs[key] = value
    
    main(action, **kwargs)