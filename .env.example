# OpenAI設定（既存）
OPENAI_API_KEY=your_openai_api_key

# Pinecone設定（既存）
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_HOST=your_pinecone_index_host

# LLMプロバイダー選択（新規：高速化対応）
# 選択肢: openai, ai_studio, bedrock
LLM_PROVIDER=openai

# LLMモデル選択（新規：詳細設定）
# OpenAI: gpt4o_mini, gpt4o, gpt35_turbo
# Google AI Studio: gemini_pro, gemini_flash, gemini_15_pro
# Bedrock: claude_haiku, claude_sonnet, titan_text, titan_text_lite
LLM_MODEL=gpt4o_mini

# Google AI Studio設定（Google Gemini API）- 高速化用
GEMINI_API_KEY=your_gemini_api_key

# AWS Bedrock設定（Amazon）- 高速化用
AWS_REGION=ap-northeast-1
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key

# 使用例：
# 最高速度重視の場合:
# LLM_PROVIDER=ai_studio
# LLM_MODEL=gemini_flash
# GEMINI_API_KEY=your_api_key

# AWS高速モデルを使用する場合:
# LLM_PROVIDER=bedrock
# LLM_MODEL=claude_haiku
# AWS_REGION=ap-northeast-1

# コスト重視の場合:
# LLM_PROVIDER=bedrock
# LLM_MODEL=titan_text_lite

# 利用可能なモデル一覧を確認:
# python llm_config.py